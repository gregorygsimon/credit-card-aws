{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T19:27:43.353860Z",
     "start_time": "2020-02-28T19:27:43.335124Z"
    }
   },
   "source": [
    "**Goal**: predict user-defined spending category based on text summaries of search results of the business name provided by credit card\n",
    "\n",
    "As this is only valuable for the user if correct, **accuracy** is the best metric of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 9 categories, and only using the preview text of the google search, we are able to achieve \n",
    "**74% test accuracy** using **TF-IDF embedding and `SGDClassifier`**, saved as `tfidf-SGD-hinge.pkl`. Since most transactions will be at places already seen by the user, this level of accuracy is already helpful to the user in categorizing their transactions.\n",
    "\n",
    "After gathering more than ~100 expenses, or by more elaborately web scraping the full results instead of just the preview text, this could likely be improved upon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T17:56:35.331767Z",
     "start_time": "2020-03-01T17:56:35.303684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (598, 3) \n",
      "\n",
      "category         object\n",
      "search_string    object\n",
      "text             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('training_df.csv')[['category','search_string','text','search']]\n",
    "\n",
    "print('shape: ',df.shape,'\\n')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T03:39:25.732730Z",
     "start_time": "2020-03-01T03:39:25.725801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>search_string</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>dining</td>\n",
       "      <td>at BOBCAT BONNIES           DETROIT      MI</td>\n",
       "      <td>Zacharie Stephen Bobcat Bonnie's. 1800 Michiga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                search_string  \\\n",
       "258   dining  at BOBCAT BONNIES           DETROIT      MI   \n",
       "\n",
       "                                                  text  \n",
       "258  Zacharie Stephen Bobcat Bonnie's. 1800 Michiga...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T16:53:18.572905Z",
     "start_time": "2020-03-01T16:53:18.544031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nulls by column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category         0\n",
       "search_string    0\n",
       "text             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('number of nulls by column:')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns:\n",
    "* **search_string** text provided by credit card on where transaction occurred\n",
    "* **text** - website summaries, from one of top google searches of search_string*\n",
    "* **category** labeled according to one of the following spending categories (notice class imbalance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T16:53:21.615563Z",
     "start_time": "2020-03-01T16:53:21.590630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dining            214\n",
       "prof dev           82\n",
       "groceries          55\n",
       "misc               46\n",
       "house              45\n",
       "transportation     44\n",
       "fun                41\n",
       "recurring          41\n",
       "pets               30\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_counts = df['category'].value_counts()\n",
    "median_count = int(val_counts.median())\n",
    "val_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance will need to be accounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploratory model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the small number of features, TF-IDF may struggle to have enough matches to differentiate classes. Probably using a pretrained word embedding will yield best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T17:56:38.547577Z",
     "start_time": "2020-03-01T17:56:38.543211Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['category'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=222)\n",
    "\n",
    "def assess(clf):\n",
    "    # function used to assess final model\n",
    "    preds = clf.predict(X_test)\n",
    "    print('test accuracy:',accuracy_score(y_test,preds))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basline: undersample majority class > tf-idf > naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T03:04:11.528665Z",
     "start_time": "2020-03-04T03:04:11.467198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 accuracy: 0.6744186046511628\n",
      "AUC averaged, one vs rest: 0.9088571095392288\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "dining_small = df[df['category']=='dining'].sample(median_count,random_state=22)\n",
    "\n",
    "resampled = pd.concat([dining_small,df[df['category']!='dining']],ignore_index=True)\n",
    "\n",
    "#bal prefix for balanced\n",
    "balX = resampled['text'].values\n",
    "baly = resampled['category'].values\n",
    "\n",
    "balX_train, balX_test, baly_train, baly_test = train_test_split(balX, baly, test_size=0.3, random_state=222)\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB(alpha=0.01))\n",
    "                    ])\n",
    "\n",
    "text_clf.fit(balX_train, baly_train)\n",
    "\n",
    "preds = text_clf.predict(balX_test)\n",
    "probs = text_clf.predict_proba(balX_test)\n",
    "\n",
    "print('                 accuracy:',accuracy_score(baly_test,preds))\n",
    "print('AUC averaged, one vs rest:',roc_auc_score(baly_test,probs,multi_class='ovr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf > Linear SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T03:10:07.563681Z",
     "start_time": "2020-03-04T03:09:56.194388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 144 candidates, totalling 1152 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:    8.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.713\n",
      "Best parameters set:\n",
      "\tclf__max_iter: 200\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "test accuracy: 0.7444444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1152 out of 1152 | elapsed:   11.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 100, 200, 300),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': ([200,400,1000]),\n",
    "    'clf__penalty': ('l2', 'elasticnet')\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,verbose=1,cv=8)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "print()\n",
    "assess(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf > SGDClassifier (logistic, SVC, Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T03:23:06.994736Z",
     "start_time": "2020-03-04T03:21:43.037527Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1728 candidates, totalling 8640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 427 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 777 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1227 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1777 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2604 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=-1)]: Done 5604 tasks      | elapsed:   59.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.749\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n",
      "\tclf__l1_ratio: 0.2\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__max_iter: 300\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "\n",
      "test accuracy: 0.7444444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 8640 out of 8640 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.4, 0.5, 0.75),\n",
    "    'vect__max_features': (None, 500),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__loss': ('hinge', 'log', 'squared_hinge', 'perceptron'),\n",
    "    'clf__max_iter': ([50,100,300]),\n",
    "    'clf__alpha': (0.01,0.005, 0.001),\n",
    "    'clf__penalty': ['elasticnet'],\n",
    "    'clf__l1_ratio': (0.01,0.05,0.1,0.2)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,verbose=1,cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "assess(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'hinge' was the best hyperparameter choice for the loss, and since 'hinge' is just a stochastic version of LinearSVC, this is very similar to the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T03:23:17.715322Z",
     "start_time": "2020-03-04T03:23:17.433278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf-SGD-hinge.pkl']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save best performing model\n",
    "import joblib\n",
    "joblib.dump(grid_search.best_estimator_,'tfidf-SGD-hinge.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## tfidf >> XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T02:51:57.236993Z",
     "start_time": "2020-03-04T02:50:22.152602Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                      validate_parameters=False,\n",
       "                                                      verbosity=None))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'clf__learning_rate': (0.001, 0.01, 0.1),\n",
       "                         'clf__max_depth': [2, 4],\n",
       "                         'clf__n_estimators': (30, 50),\n",
       "                         'vect__max_df': (0.5, 0.75),\n",
       "                         'vect__max_features': (None, 500),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', XGBClassifier())\n",
    "     ])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75),\n",
    "    'vect__max_features': (None, 500),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_depth': ([2,4]),\n",
    "    'clf__n_estimators': (30,50),\n",
    "    'clf__learning_rate': (0.001,0.01,0.1),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,verbose=1,cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "#print(\"Best parameters set:\")\n",
    "#best_parameters = grid_search.best_estimator_.get_params()\n",
    "#for param_name in sorted(parameters.keys()):\n",
    "#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))   \n",
    "#print('\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T02:52:17.615293Z",
     "start_time": "2020-03-04T02:52:17.600844Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.5166666666666667\n"
     ]
    }
   ],
   "source": [
    "preds = grid_search.predict(X_test)\n",
    "print('test accuracy:',accuracy_score(y_test,preds))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T02:52:23.166434Z",
     "start_time": "2020-03-04T02:52:23.161202Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.581\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.1\n",
      "\tclf__max_depth: 4\n",
      "\tclf__n_estimators: 30\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: 500\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))   \n",
    "print('\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## word2vec pretrained > SGDClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Without enough data to train a doc2vec model, one approach is to average the word2vec vectors on a pretrained word2vec model to get the embedding. Members of the Google team behind word2vec mentions this as an option for tasks where the word order is less important [(Le and Mikolov, 2014)](https://cs.stanford.edu/~quocle/paragraph_vector.pdf). The ability for pretrained model to know similarity between, e.g., 'taco' and 'sandwich' should help the model classify these terms within similar spending categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T17:57:41.206327Z",
     "start_time": "2020-03-01T17:56:50.048422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '/media/hdd_1tb/data/GoogleNews-vectors-negative300.bin', binary=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T17:57:41.286788Z",
     "start_time": "2020-03-01T17:57:41.214412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def avgWords2Vec(sent):\n",
    "    # averages word2vec vectors in sentence \n",
    "    sent = gensim.parsing.remove_stopwords(sent)\n",
    "    lemmas = gensim.utils.simple_preprocess(sent)\n",
    "    vecs = []\n",
    "    for lem in lemmas:\n",
    "        try:\n",
    "            vecs.append(model.get_vector(lem))\n",
    "        except KeyError:\n",
    "            # word not in pretrained vocab\n",
    "            None\n",
    "    return np.mean(vecs,axis=0)\n",
    "\n",
    "embedded_train = np.array([avgWords2Vec(sent) for sent in X_train])\n",
    "embedded_test = np.array([avgWords2Vec(sent) for sent in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### visualizing word2vec embedding with PCA and t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T18:33:00.433627Z",
     "start_time": "2020-03-01T18:32:59.772194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as psx\n",
    "\n",
    "vis1 = PCA(n_components=3).fit_transform(embedded_train)\n",
    "\n",
    "fig = psx.scatter_3d(x=vis1[:,0],y=vis1[:,1],z=vis1[:,2],color=y_train)\n",
    "fig.update_layout(title='3D visualization of PCA of 300-dimensional word2vec embedding')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T18:33:55.948363Z",
     "start_time": "2020-03-01T18:33:48.513074Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 418 samples in 0.008s...\n",
      "[t-SNE] Computed neighbors for 418 samples in 0.107s...\n",
      "[t-SNE] Computed conditional probabilities for sample 418 / 418\n",
      "[t-SNE] Mean sigma: 0.296016\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 145.531570\n",
      "[t-SNE] KL divergence after 3000 iterations: 1.966193\n"
     ]
    }
   ],
   "source": [
    "vis2 = TSNE(n_components=3,perplexity=30,n_jobs=-1,verbose=1,n_iter=3000).fit_transform(\n",
    "        embedded_train\n",
    ")\n",
    "\n",
    "import plotly.express as psx\n",
    "\n",
    "fig = psx.scatter_3d(x=vis2[:,0],y=vis2[:,1],z=vis2[:,2],color=y_train)\n",
    "fig.update_layout(title='3D visualization using t-SNE of 300-dimensional word2vec embedding')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T02:27:52.962120Z",
     "start_time": "2020-03-04T02:27:52.955549Z"
    },
    "hidden": true
   },
   "source": [
    "Even after several modifications of the perplexity, no clear clusters emerging from the t-SNE visualization. This could be a non-issue: the full 300 dimensional data may differentiate the categories which dissappears in the 3D represntation. However if tSNE cannot find any correct clusterss this could bode poorly for the potential of the word2vec embedding in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T18:30:46.418934Z",
     "start_time": "2020-03-01T18:30:45.657577Z"
    },
    "hidden": true
   },
   "source": [
    "### word2vec > SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T18:40:46.674627Z",
     "start_time": "2020-03-01T18:39:57.993027Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 223 tasks      | elapsed:   38.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.610\n",
      "Best parameters set:\n",
      "\talpha: 0.001\n",
      "\tl1_ratio: 0.01\n",
      "\tloss: 'hinge'\n",
      "\tmax_iter: 1000\n",
      "\tpenalty: 'elasticnet'\n",
      "\n",
      "\n",
      "test accuracy: 0.5555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   48.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'loss': ('hinge', 'log', 'squared_hinge', 'perceptron'),\n",
    "    'alpha': (0.01,0.005, 0.001),\n",
    "    'penalty': ['elasticnet'],\n",
    "    'l1_ratio': (0.01,0.05,0.1,0.2),\n",
    "    'max_iter': (1000,2000),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(sgd, parameters, n_jobs=-1,verbose=1,cv=5)\n",
    "grid_search.fit(embedded_train, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "preds = grid_search.predict(embedded_test)\n",
    "print('test accuracy:',accuracy_score(y_test,preds))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T02:20:11.761972Z",
     "start_time": "2020-03-04T02:20:11.757220Z"
    },
    "hidden": true
   },
   "source": [
    "The averaged, pretrained word2vec embedding is not proving helpful for feature extraction. One possible reason: including too many search results waters down the relevance of the data. Can test this hypothesis by running the same experiment but limiting to the top 2 search results rather than the top 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## limiting search terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Possibly using the top 5 search results leads to watered down data, which may make it harder for models to differentiate. To investigate this hypothesis, we reload the dataset only including the top 3 search results rather than the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T02:57:25.807894Z",
     "start_time": "2020-03-04T02:57:25.798080Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 4)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ldf = pd.read_csv('training_df.csv')\n",
    "Ldf = Ldf[Ldf['search_ranking']<3]\n",
    "Ldf.shape # Limited data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T02:57:27.459511Z",
     "start_time": "2020-03-04T02:57:27.397645Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LX = df['text'].values\n",
    "Ly = df['category'].values\n",
    "\n",
    "LX_train, LX_test, Ly_train, Ly_test = train_test_split(LX, Ly, test_size=0.3, random_state=222)\n",
    "\n",
    "Lembedded_train = np.array([avgWords2Vec(sent) for sent in LX_train])\n",
    "Lembedded_test = np.array([avgWords2Vec(sent) for sent in LX_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T02:58:53.961145Z",
     "start_time": "2020-03-04T02:57:28.618961Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:   53.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.610\n",
      "Best parameters set:\n",
      "\talpha: 0.001\n",
      "\tl1_ratio: 0.1\n",
      "\tloss: 'hinge'\n",
      "\tmax_iter: 2000\n",
      "\tpenalty: 'elasticnet'\n",
      "\n",
      "\n",
      "test accuracy: 0.6111111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'loss': ('hinge', 'log', 'squared_hinge', 'perceptron'),\n",
    "    'alpha': (0.01,0.005, 0.001),\n",
    "    'penalty': ['elasticnet'],\n",
    "    'l1_ratio': (0.01,0.05,0.1,0.2),\n",
    "    'max_iter': (1000,2000),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(sgd, parameters, n_jobs=-1,verbose=1,cv=5)\n",
    "grid_search.fit(Lembedded_train, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "preds = grid_search.predict(Lembedded_test)\n",
    "print('test accuracy:',accuracy_score(Ly_test,preds))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
